{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Purpose of first model\n",
    "This first, simple, prelimiary model take the ~2700 songs from the US database and attempt to see which appoach leads to the most meaningful result.\n",
    "\n",
    "First, clean up lyrics by removing non-alphanumeric values, punctation, and then performing lemmatizing\n",
    "\n",
    "Try two different vectorizers\n",
    "1) CountVectorizer\n",
    "2) TF-TDF vectorizers\n",
    "\n",
    "Try three different topic modeling techniques\n",
    "1) Truncated SVD (LSA)\n",
    "2) NMF\n",
    "3) LDA\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('../../Webscraper/us_pops_raw', 'rb')\n",
    "us_pops = pickle.load(file)\n",
    "us_pops.dropna(inplace=True)\n",
    "us_pops = us_pops['Lyrics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0       I know this pain (I know this pain)\\nWhy do yo...\n",
       "1       Lay a whisper on my pillow\\nLeave the winter o...\n",
       "2       It's been seven hours and fifteen days\\nSince ...\n",
       "3       Yeah, Spyderman and Freeze in full effect\\nUh-...\n",
       "4       Strike a pose\\nStrike a pose\\nVogue (vogue, vo...\n",
       "                              ...                        \n",
       "2750    We've been to both Carolinas\\nSeen a big Monta...\n",
       "2751    I'm jealous of the blue jeans that you're wear...\n",
       "2752    I'm a motherfuckin' train wreck\\nI don't wanna...\n",
       "2753    There's somethin' in the way you roll your eye...\n",
       "2754    Man, what? (Haha)\\nThis shit funny, one sec\\nO...\n",
       "Name: Lyrics, Length: 2671, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "us_pops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "### 1. Clean up text for NLP"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only keep alphanumeric, remove newlines, and make everything lowercase\n",
    "alphanumeric = lambda x: re.sub('\\w*\\d\\w*', ' ', x)\n",
    "rm_newline = lambda x: re.sub(\"\\n\", ' ',x)\n",
    "punc_lower = lambda x: re.sub('[%s]' % re.escape(string.punctuation), ' ', x.lower())\n",
    "\n",
    "us_pops_nopunc = us_pops.map(alphanumeric).map(rm_newline).map(punc_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatize. do this by turning every line into a list of words, and concat back to full lyrics\n",
    "us_pops_list = us_pops_nopunc.apply(lambda x: x.split(' '))\n",
    "\n",
    "def lemmatize(list_of_words):\n",
    "    lmt = WordNetLemmatizer()\n",
    "    strg = ''\n",
    "    for word in list_of_words:\n",
    "        lem_word = lmt.lemmatize(word)\n",
    "        if len(lem_word) > 2:\n",
    "            strg += lem_word + ' '\n",
    "    return strg\n",
    "        \n",
    "\n",
    "us_pops_clean = us_pops_list.apply(lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "### 2. Create two vectorized formats, CountVectorizer and TFIDF vectorizer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_added = text.ENGLISH_STOP_WORDS.union(['huh','woo','whoa','hey','hold']) #these words added through iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(\n",
    "    stop_words = 'english',\n",
    "    max_df = 0.2,\n",
    "    min_df = 0.0005\n",
    ")\n",
    "X = cv.fit_transform(us_pops_clean)\n",
    "cv_doc_term_matrix = pd.DataFrame(X.toarray(), columns=cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "      aaa  aah  aaliyah  aback  abandoned  abashed  abc  abdul  abeat  abel  \\\n",
       "0       0    0        0      0          0        0    0      0      0     0   \n",
       "1       0    0        0      0          0        0    0      0      0     0   \n",
       "2       0    0        0      0          0        0    0      0      0     0   \n",
       "3       1    0        0      0          0        0    0      0      0     0   \n",
       "4       0    0        0      0          0        0    0      0      0     0   \n",
       "...   ...  ...      ...    ...        ...      ...  ...    ...    ...   ...   \n",
       "2666    0    0        0      0          0        0    0      0      0     0   \n",
       "2667    0    0        0      0          0        0    0      0      0     0   \n",
       "2668    0    0        0      0          0        0    0      0      0     0   \n",
       "2669    0    0        0      0          0        0    0      0      0     0   \n",
       "2670    0    0        0      0          0        0    0      0      0     0   \n",
       "\n",
       "      ...  zulu  zurück  çünkü  étais  était  être  über  üstünde  şey  şimdi  \n",
       "0     ...     0       0      0      0      0     0     0        0    0      0  \n",
       "1     ...     0       0      0      0      0     0     0        0    0      0  \n",
       "2     ...     0       0      0      0      0     0     0        0    0      0  \n",
       "3     ...     0       0      0      0      0     0     0        0    0      0  \n",
       "4     ...     0       0      0      0      0     0     0        0    0      0  \n",
       "...   ...   ...     ...    ...    ...    ...   ...   ...      ...  ...    ...  \n",
       "2666  ...     0       0      0      0      0     0     0        0    0      0  \n",
       "2667  ...     0       0      0      0      0     0     0        0    0      0  \n",
       "2668  ...     0       0      0      0      0     0     0        0    0      0  \n",
       "2669  ...     0       0      0      0      0     0     0        0    0      0  \n",
       "2670  ...     0       0      0      0      0     0     0        0    0      0  \n",
       "\n",
       "[2671 rows x 12998 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>aaa</th>\n      <th>aah</th>\n      <th>aaliyah</th>\n      <th>aback</th>\n      <th>abandoned</th>\n      <th>abashed</th>\n      <th>abc</th>\n      <th>abdul</th>\n      <th>abeat</th>\n      <th>abel</th>\n      <th>...</th>\n      <th>zulu</th>\n      <th>zurück</th>\n      <th>çünkü</th>\n      <th>étais</th>\n      <th>était</th>\n      <th>être</th>\n      <th>über</th>\n      <th>üstünde</th>\n      <th>şey</th>\n      <th>şimdi</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2666</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2667</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2668</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2669</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2670</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>2671 rows × 12998 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "cv_doc_term_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf =  TfidfVectorizer(\n",
    "    stop_words = stop_words_added,\n",
    "    max_df = 0.2,\n",
    "    min_df = 0.0005\n",
    ")\n",
    "Y = tfidf.fit_transform(us_pops_clean)\n",
    "tfidf_doc_term_matrix = pd.DataFrame(Y.toarray(), columns=tfidf.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "           aaa  aah  aaliyah  aback  abandoned  abashed  abc  abdul  abeat  \\\n",
       "0     0.000000  0.0      0.0    0.0        0.0      0.0  0.0    0.0    0.0   \n",
       "1     0.000000  0.0      0.0    0.0        0.0      0.0  0.0    0.0    0.0   \n",
       "2     0.000000  0.0      0.0    0.0        0.0      0.0  0.0    0.0    0.0   \n",
       "3     0.046599  0.0      0.0    0.0        0.0      0.0  0.0    0.0    0.0   \n",
       "4     0.000000  0.0      0.0    0.0        0.0      0.0  0.0    0.0    0.0   \n",
       "...        ...  ...      ...    ...        ...      ...  ...    ...    ...   \n",
       "2666  0.000000  0.0      0.0    0.0        0.0      0.0  0.0    0.0    0.0   \n",
       "2667  0.000000  0.0      0.0    0.0        0.0      0.0  0.0    0.0    0.0   \n",
       "2668  0.000000  0.0      0.0    0.0        0.0      0.0  0.0    0.0    0.0   \n",
       "2669  0.000000  0.0      0.0    0.0        0.0      0.0  0.0    0.0    0.0   \n",
       "2670  0.000000  0.0      0.0    0.0        0.0      0.0  0.0    0.0    0.0   \n",
       "\n",
       "      abel  ...  zulu  zurück  çünkü  étais  était  être  über  üstünde  şey  \\\n",
       "0      0.0  ...   0.0     0.0    0.0    0.0    0.0   0.0   0.0      0.0  0.0   \n",
       "1      0.0  ...   0.0     0.0    0.0    0.0    0.0   0.0   0.0      0.0  0.0   \n",
       "2      0.0  ...   0.0     0.0    0.0    0.0    0.0   0.0   0.0      0.0  0.0   \n",
       "3      0.0  ...   0.0     0.0    0.0    0.0    0.0   0.0   0.0      0.0  0.0   \n",
       "4      0.0  ...   0.0     0.0    0.0    0.0    0.0   0.0   0.0      0.0  0.0   \n",
       "...    ...  ...   ...     ...    ...    ...    ...   ...   ...      ...  ...   \n",
       "2666   0.0  ...   0.0     0.0    0.0    0.0    0.0   0.0   0.0      0.0  0.0   \n",
       "2667   0.0  ...   0.0     0.0    0.0    0.0    0.0   0.0   0.0      0.0  0.0   \n",
       "2668   0.0  ...   0.0     0.0    0.0    0.0    0.0   0.0   0.0      0.0  0.0   \n",
       "2669   0.0  ...   0.0     0.0    0.0    0.0    0.0   0.0   0.0      0.0  0.0   \n",
       "2670   0.0  ...   0.0     0.0    0.0    0.0    0.0   0.0   0.0      0.0  0.0   \n",
       "\n",
       "      şimdi  \n",
       "0       0.0  \n",
       "1       0.0  \n",
       "2       0.0  \n",
       "3       0.0  \n",
       "4       0.0  \n",
       "...     ...  \n",
       "2666    0.0  \n",
       "2667    0.0  \n",
       "2668    0.0  \n",
       "2669    0.0  \n",
       "2670    0.0  \n",
       "\n",
       "[2671 rows x 12993 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>aaa</th>\n      <th>aah</th>\n      <th>aaliyah</th>\n      <th>aback</th>\n      <th>abandoned</th>\n      <th>abashed</th>\n      <th>abc</th>\n      <th>abdul</th>\n      <th>abeat</th>\n      <th>abel</th>\n      <th>...</th>\n      <th>zulu</th>\n      <th>zurück</th>\n      <th>çünkü</th>\n      <th>étais</th>\n      <th>était</th>\n      <th>être</th>\n      <th>über</th>\n      <th>üstünde</th>\n      <th>şey</th>\n      <th>şimdi</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.046599</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2666</th>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2667</th>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2668</th>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2669</th>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2670</th>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>2671 rows × 12993 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "tfidf_doc_term_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "### 3. Use the two vectorizers above to feed into LSA, NMF, and LDA\n",
    "### LSA below"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words, topic_names=None):\n",
    "    for ix, topic in enumerate(model.components_):\n",
    "        if not topic_names or not topic_names[ix]:\n",
    "            print(\"\\nTopic \", ix)\n",
    "        else:\n",
    "            print(\"\\nTopic: '\",topic_names[ix],\"'\")\n",
    "        print(\", \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "        aaa    aah  aaliyah  aback  abandoned  abashed  abc  abdul  abeat  \\\n",
       "topic1  0.0  0.000      0.0  0.001      0.003    0.001  0.0    0.0  0.000   \n",
       "topic2  0.0  0.000      0.0 -0.000      0.000    0.000  0.0    0.0  0.000   \n",
       "topic3  0.0  0.000      0.0 -0.000      0.000    0.000  0.0    0.0  0.000   \n",
       "topic4  0.0  0.001      0.0 -0.000      0.004    0.002  0.0    0.0  0.000   \n",
       "topic5  0.0  0.003      0.0  0.000     -0.002   -0.001  0.0    0.0  0.001   \n",
       "\n",
       "        abel  ...   zulu  zurück  çünkü  étais  était  être   über  üstünde  \\\n",
       "topic1   0.0  ...  0.000   0.000    0.0    0.0    0.0   0.0  0.000      0.0   \n",
       "topic2   0.0  ...  0.000   0.000    0.0    0.0    0.0   0.0  0.000      0.0   \n",
       "topic3   0.0  ...  0.000   0.000    0.0    0.0    0.0   0.0  0.000      0.0   \n",
       "topic4   0.0  ...  0.001   0.000    0.0    0.0    0.0   0.0  0.000      0.0   \n",
       "topic5   0.0  ...  0.004   0.001    0.0    0.0    0.0   0.0  0.001      0.0   \n",
       "\n",
       "        şey  şimdi  \n",
       "topic1  0.0    0.0  \n",
       "topic2  0.0    0.0  \n",
       "topic3  0.0    0.0  \n",
       "topic4  0.0    0.0  \n",
       "topic5  0.0    0.0  \n",
       "\n",
       "[5 rows x 12998 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>aaa</th>\n      <th>aah</th>\n      <th>aaliyah</th>\n      <th>aback</th>\n      <th>abandoned</th>\n      <th>abashed</th>\n      <th>abc</th>\n      <th>abdul</th>\n      <th>abeat</th>\n      <th>abel</th>\n      <th>...</th>\n      <th>zulu</th>\n      <th>zurück</th>\n      <th>çünkü</th>\n      <th>étais</th>\n      <th>était</th>\n      <th>être</th>\n      <th>über</th>\n      <th>üstünde</th>\n      <th>şey</th>\n      <th>şimdi</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>topic1</th>\n      <td>0.0</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>0.001</td>\n      <td>0.003</td>\n      <td>0.001</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>topic2</th>\n      <td>0.0</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>-0.000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>topic3</th>\n      <td>0.0</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>-0.000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>topic4</th>\n      <td>0.0</td>\n      <td>0.001</td>\n      <td>0.0</td>\n      <td>-0.000</td>\n      <td>0.004</td>\n      <td>0.002</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.001</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>topic5</th>\n      <td>0.0</td>\n      <td>0.003</td>\n      <td>0.0</td>\n      <td>0.000</td>\n      <td>-0.002</td>\n      <td>-0.001</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.001</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.004</td>\n      <td>0.001</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.001</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 12998 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "# LSA with CountVectorizer\n",
    "\n",
    "lsa_cv = TruncatedSVD(5)\n",
    "doc_topic = lsa_cv.fit_transform(cv_doc_term_matrix)\n",
    "# lsa.explained_variance_ratio_\n",
    "\n",
    "topic_word = pd.DataFrame(\n",
    "    lsa_cv.components_.round(3), \n",
    "    index = ['topic1','topic2','topic3','topic4','topic5'],\n",
    "    columns = cv.get_feature_names()\n",
    "    )\n",
    "topic_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nTopic  0\nsaint, did, friend, moment, people, room, word, quite, grandmother, great\n\nTopic  1\ndoo, shark, shoo, woman, alright, wild, juan, looking, run, doop\n\nTopic  2\nchoo, ride, woo, train, juan, nigga, ana, hey, devil, statue\n\nTopic  3\njuan, ana, devil, statue, woman, heaven, tanner, old, hell, men\n\nTopic  4\nnigga, hey, bitch, shit, fuck, featuring, low, money, rock, gotta\n"
     ]
    }
   ],
   "source": [
    "display_topics(lsa_cv, cv.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "          aaa    aah  aaliyah  aback  abandoned  abashed  abc  abdul  abeat  \\\n",
       "topic1  0.001  0.006    0.001    0.0      0.001      0.0  0.0    0.0    0.0   \n",
       "topic2 -0.001 -0.003    0.001   -0.0     -0.001     -0.0  0.0   -0.0   -0.0   \n",
       "topic3 -0.001 -0.000   -0.000   -0.0     -0.001     -0.0 -0.0   -0.0   -0.0   \n",
       "topic4 -0.000 -0.005   -0.001   -0.0      0.001     -0.0 -0.0   -0.0   -0.0   \n",
       "topic5 -0.002 -0.007    0.001   -0.0     -0.001     -0.0  0.0   -0.0   -0.0   \n",
       "\n",
       "         abel  ...  zulu  zurück  çünkü  étais  était  être   über  üstünde  \\\n",
       "topic1  0.000  ...   0.0   0.000    0.0    0.0    0.0   0.0  0.000      0.0   \n",
       "topic2 -0.000  ...   0.0   0.001    0.0    0.0    0.0   0.0 -0.000      0.0   \n",
       "topic3 -0.000  ...   0.0   0.000    0.0    0.0    0.0   0.0 -0.000      0.0   \n",
       "topic4 -0.000  ...  -0.0  -0.000   -0.0   -0.0   -0.0  -0.0  0.000     -0.0   \n",
       "topic5 -0.001  ...  -0.0  -0.001   -0.0   -0.0   -0.0  -0.0 -0.001     -0.0   \n",
       "\n",
       "        şey  şimdi  \n",
       "topic1  0.0    0.0  \n",
       "topic2  0.0    0.0  \n",
       "topic3  0.0   -0.0  \n",
       "topic4 -0.0   -0.0  \n",
       "topic5 -0.0   -0.0  \n",
       "\n",
       "[5 rows x 12993 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>aaa</th>\n      <th>aah</th>\n      <th>aaliyah</th>\n      <th>aback</th>\n      <th>abandoned</th>\n      <th>abashed</th>\n      <th>abc</th>\n      <th>abdul</th>\n      <th>abeat</th>\n      <th>abel</th>\n      <th>...</th>\n      <th>zulu</th>\n      <th>zurück</th>\n      <th>çünkü</th>\n      <th>étais</th>\n      <th>était</th>\n      <th>être</th>\n      <th>über</th>\n      <th>üstünde</th>\n      <th>şey</th>\n      <th>şimdi</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>topic1</th>\n      <td>0.001</td>\n      <td>0.006</td>\n      <td>0.001</td>\n      <td>0.0</td>\n      <td>0.001</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>topic2</th>\n      <td>-0.001</td>\n      <td>-0.003</td>\n      <td>0.001</td>\n      <td>-0.0</td>\n      <td>-0.001</td>\n      <td>-0.0</td>\n      <td>0.0</td>\n      <td>-0.0</td>\n      <td>-0.0</td>\n      <td>-0.000</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.001</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>topic3</th>\n      <td>-0.001</td>\n      <td>-0.000</td>\n      <td>-0.000</td>\n      <td>-0.0</td>\n      <td>-0.001</td>\n      <td>-0.0</td>\n      <td>-0.0</td>\n      <td>-0.0</td>\n      <td>-0.0</td>\n      <td>-0.000</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.0</td>\n    </tr>\n    <tr>\n      <th>topic4</th>\n      <td>-0.000</td>\n      <td>-0.005</td>\n      <td>-0.001</td>\n      <td>-0.0</td>\n      <td>0.001</td>\n      <td>-0.0</td>\n      <td>-0.0</td>\n      <td>-0.0</td>\n      <td>-0.0</td>\n      <td>-0.000</td>\n      <td>...</td>\n      <td>-0.0</td>\n      <td>-0.000</td>\n      <td>-0.0</td>\n      <td>-0.0</td>\n      <td>-0.0</td>\n      <td>-0.0</td>\n      <td>0.000</td>\n      <td>-0.0</td>\n      <td>-0.0</td>\n      <td>-0.0</td>\n    </tr>\n    <tr>\n      <th>topic5</th>\n      <td>-0.002</td>\n      <td>-0.007</td>\n      <td>0.001</td>\n      <td>-0.0</td>\n      <td>-0.001</td>\n      <td>-0.0</td>\n      <td>0.0</td>\n      <td>-0.0</td>\n      <td>-0.0</td>\n      <td>-0.001</td>\n      <td>...</td>\n      <td>-0.0</td>\n      <td>-0.001</td>\n      <td>-0.0</td>\n      <td>-0.0</td>\n      <td>-0.0</td>\n      <td>-0.0</td>\n      <td>-0.001</td>\n      <td>-0.0</td>\n      <td>-0.0</td>\n      <td>-0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 12993 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "# LSA with TF-IDF\n",
    "\n",
    "lsa_tfidf = TruncatedSVD(5)\n",
    "doc_topic = lsa_tfidf.fit_transform(tfidf_doc_term_matrix)\n",
    "# lsa.explained_variance_ratio_\n",
    "\n",
    "topic_word = pd.DataFrame(\n",
    "    lsa_tfidf.components_.round(3), \n",
    "    index = ['topic1','topic2','topic3','topic4','topic5'],\n",
    "    columns = tfidf.get_feature_names()\n",
    "    )\n",
    "topic_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nTopic  0\nnigga, tonight, gotta, really, long, bitch, better, shit, turn, friend\n\nTopic  1\nnigga, bitch, shit, fuck, ayy, money, gon, lil, hoe, hit\n\nTopic  2\ntonight, body, dance, party, alright, gotta, shake, rock, floor, everybody\n\nTopic  3\ntonight, nigga, bitch, fuck, dream, believe, forever, shit, promise, true\n\nTopic  4\nreally, gotta, bad, better, promise, babe, friend, stay, real, sorry\n"
     ]
    }
   ],
   "source": [
    "display_topics(lsa_tfidf, tfidf.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "### NMF below"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "     aaa    aah  aaliyah  aback  abandoned  abashed    abc  abdul  abeat  \\\n",
       "0  0.000  0.001    0.000  0.029      0.110    0.027  0.000  0.001  0.002   \n",
       "1  0.000  0.000    0.000  0.000      0.000    0.000  0.000  0.000  0.000   \n",
       "2  0.000  0.000    0.000  0.000      0.000    0.000  0.000  0.000  0.000   \n",
       "3  0.001  0.001    0.000  0.002      0.098    0.049  0.001  0.003  0.004   \n",
       "4  0.001  0.048    0.005  0.005      0.000    0.000  0.003  0.007  0.011   \n",
       "\n",
       "    abel  ...   zulu  zurück  çünkü  étais  était   être   über  üstünde  \\\n",
       "0  0.001  ...  0.000    0.00    0.0    0.0    0.0  0.000  0.000      0.0   \n",
       "1  0.000  ...  0.000    0.00    0.0    0.0    0.0  0.000  0.000      0.0   \n",
       "2  0.000  ...  0.000    0.00    0.0    0.0    0.0  0.000  0.000      0.0   \n",
       "3  0.002  ...  0.000    0.00    0.0    0.0    0.0  0.000  0.002      0.0   \n",
       "4  0.007  ...  0.075    0.02    0.0    0.0    0.0  0.001  0.014      0.0   \n",
       "\n",
       "     şey  şimdi  \n",
       "0  0.000  0.000  \n",
       "1  0.000  0.000  \n",
       "2  0.000  0.000  \n",
       "3  0.000  0.000  \n",
       "4  0.001  0.001  \n",
       "\n",
       "[5 rows x 12998 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>aaa</th>\n      <th>aah</th>\n      <th>aaliyah</th>\n      <th>aback</th>\n      <th>abandoned</th>\n      <th>abashed</th>\n      <th>abc</th>\n      <th>abdul</th>\n      <th>abeat</th>\n      <th>abel</th>\n      <th>...</th>\n      <th>zulu</th>\n      <th>zurück</th>\n      <th>çünkü</th>\n      <th>étais</th>\n      <th>était</th>\n      <th>être</th>\n      <th>über</th>\n      <th>üstünde</th>\n      <th>şey</th>\n      <th>şimdi</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.000</td>\n      <td>0.001</td>\n      <td>0.000</td>\n      <td>0.029</td>\n      <td>0.110</td>\n      <td>0.027</td>\n      <td>0.000</td>\n      <td>0.001</td>\n      <td>0.002</td>\n      <td>0.001</td>\n      <td>...</td>\n      <td>0.000</td>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>0.000</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>...</td>\n      <td>0.000</td>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>0.000</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>...</td>\n      <td>0.000</td>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>0.000</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.001</td>\n      <td>0.001</td>\n      <td>0.000</td>\n      <td>0.002</td>\n      <td>0.098</td>\n      <td>0.049</td>\n      <td>0.001</td>\n      <td>0.003</td>\n      <td>0.004</td>\n      <td>0.002</td>\n      <td>...</td>\n      <td>0.000</td>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000</td>\n      <td>0.002</td>\n      <td>0.0</td>\n      <td>0.000</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.001</td>\n      <td>0.048</td>\n      <td>0.005</td>\n      <td>0.005</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.003</td>\n      <td>0.007</td>\n      <td>0.011</td>\n      <td>0.007</td>\n      <td>...</td>\n      <td>0.075</td>\n      <td>0.02</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.001</td>\n      <td>0.014</td>\n      <td>0.0</td>\n      <td>0.001</td>\n      <td>0.001</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 12998 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "# NMF with CountVectorizer\n",
    "\n",
    "nmf_cv = NMF(5)\n",
    "doc_topic = nmf_cv.fit_transform(cv_doc_term_matrix)\n",
    "\n",
    "# doc_topic_mat = pd.DataFrame(\n",
    "#     doc_topic.round(3),\n",
    "#     columns = ['topic1','topic2','topic3','topic4','topic5']\n",
    "# )\n",
    "\n",
    "topic_word = pd.DataFrame(\n",
    "    nmf_cv.components_.round(3),\n",
    "    columns = cv.get_feature_names()\n",
    "    )\n",
    "topic_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nTopic  0\nsaint, did, friend, moment, people, room, word, quite, grandmother, great, house, face, thought, having, woman\n\nTopic  1\ndoo, shark, shoo, alright, wild, woman, looking, doop, shooby, bay, run, end, west, morning, wop\n\nTopic  2\nchoo, ride, woo, train, coming, sound, getting, dancing, walk, mon, jump, pack, drive, talk, hoo\n\nTopic  3\njuan, woman, ana, devil, statue, old, heaven, tanner, friend, men, lady, hell, did, social, force\n\nTopic  4\nnigga, hey, bitch, shit, fuck, featuring, money, rock, low, gotta, bad, big, gon, beat, doh\n"
     ]
    }
   ],
   "source": [
    "display_topics(nmf_cv, cv.get_feature_names(), 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "     aaa    aah  aaliyah  aback  abandoned  abashed    abc  abdul  abeat  \\\n",
       "0  0.004  0.019    0.000    0.0      0.004      0.0  0.000  0.001  0.001   \n",
       "1  0.000  0.000    0.003    0.0      0.000      0.0  0.001  0.000  0.000   \n",
       "2  0.000  0.000    0.000    0.0      0.000      0.0  0.000  0.000  0.000   \n",
       "3  0.000  0.012    0.001    0.0      0.000      0.0  0.000  0.000  0.000   \n",
       "4  0.000  0.000    0.002    0.0      0.000      0.0  0.002  0.000  0.000   \n",
       "\n",
       "    abel  ...   zulu  zurück  çünkü  étais  était  être   über  üstünde  şey  \\\n",
       "0  0.001  ...  0.000   0.000    0.0    0.0    0.0   0.0  0.002      0.0  0.0   \n",
       "1  0.000  ...  0.000   0.003    0.0    0.0    0.0   0.0  0.000      0.0  0.0   \n",
       "2  0.000  ...  0.000   0.000    0.0    0.0    0.0   0.0  0.000      0.0  0.0   \n",
       "3  0.000  ...  0.001   0.000    0.0    0.0    0.0   0.0  0.000      0.0  0.0   \n",
       "4  0.000  ...  0.000   0.000    0.0    0.0    0.0   0.0  0.000      0.0  0.0   \n",
       "\n",
       "   şimdi  \n",
       "0    0.0  \n",
       "1    0.0  \n",
       "2    0.0  \n",
       "3    0.0  \n",
       "4    0.0  \n",
       "\n",
       "[5 rows x 12993 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>aaa</th>\n      <th>aah</th>\n      <th>aaliyah</th>\n      <th>aback</th>\n      <th>abandoned</th>\n      <th>abashed</th>\n      <th>abc</th>\n      <th>abdul</th>\n      <th>abeat</th>\n      <th>abel</th>\n      <th>...</th>\n      <th>zulu</th>\n      <th>zurück</th>\n      <th>çünkü</th>\n      <th>étais</th>\n      <th>était</th>\n      <th>être</th>\n      <th>über</th>\n      <th>üstünde</th>\n      <th>şey</th>\n      <th>şimdi</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.004</td>\n      <td>0.019</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>0.004</td>\n      <td>0.0</td>\n      <td>0.000</td>\n      <td>0.001</td>\n      <td>0.001</td>\n      <td>0.001</td>\n      <td>...</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.002</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.003</td>\n      <td>0.0</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>0.001</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>...</td>\n      <td>0.000</td>\n      <td>0.003</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>...</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.000</td>\n      <td>0.012</td>\n      <td>0.001</td>\n      <td>0.0</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>...</td>\n      <td>0.001</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.002</td>\n      <td>0.0</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>0.002</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>...</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 12993 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "# NMF with TF-IDF\n",
    "\n",
    "nmf_tfidf = NMF(5)\n",
    "doc_topic = nmf_tfidf.fit_transform(tfidf_doc_term_matrix)\n",
    "\n",
    "# doc_topic_mat = pd.DataFrame(\n",
    "#     doc_topic.round(3),\n",
    "#     columns = ['topic1','topic2','topic3','topic4','topic5']\n",
    "# )\n",
    "\n",
    "topic_word = pd.DataFrame(\n",
    "    nmf_tfidf.components_.round(3),\n",
    "    columns = tfidf.get_feature_names()\n",
    "    )\n",
    "topic_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nTopic  0\ndream, believe, gone, forever, long, remember, wish, true, fall, inside, home, face, place, better, stay\n\nTopic  1\nnigga, bitch, shit, fuck, ayy, money, gon, lil, hoe, real, big, hit, fuckin, pussy, dick\n\nTopic  2\ntonight, alright, waiting, tomorrow, fight, party, tight, kiss, inside, gotta, drink, dancing, boo, end, broken\n\nTopic  3\ndance, body, shake, rock, everybody, party, stop, turn, floor, play, music, round, roll, club, beat\n\nTopic  4\nreally, gotta, bad, somebody, woman, real, friend, yes, lover, try, babe, stay, sorry, talk, care\n"
     ]
    }
   ],
   "source": [
    "display_topics(nmf_tfidf, tfidf.get_feature_names(), 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "### LDA Below"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(random_state=0)"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "# for TF DTM\n",
    "lda_tf = LatentDirichletAllocation(n_components=10, random_state=0)\n",
    "lda_tf.fit(cv_doc_term_matrix)\n",
    "\n",
    "# for TFIDF DTM\n",
    "lda_tfidf = LatentDirichletAllocation(n_components=10, random_state=0)\n",
    "lda_tfidf.fit(tfidf_doc_term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nTopic  0\ntonight, break, friend, fall, light, going, dem, gimme, end, power, black, turn, lie, blue, clean\n\nTopic  1\ndoo, really, believe, dream, forever, hold, feeling, whoa, did, inside, better, miss, place, face, close\n\nTopic  2\ngotta, crazy, walk, kiss, talk, tonight, hold, try, yes, touch, lover, everybody, sorry, rock, hey\n\nTopic  3\nshake, gon, real, wit, step, nigga, run, long, gotta, throw, dat, hit, huh, booty, really\n\nTopic  4\nwoo, ride, que, hot, party, fly, rock, choo, whoa, roll, bye, bit, lady, hip, stop\n\nTopic  5\nhey, woman, wild, juan, ready, head, old, wee, happy, lady, bring, dee, sweet, hoo, people\n\nTopic  6\nbody, low, dance, turn, play, stop, song, music, floor, alright, fine, wish, babe, matter, hey\n\nTopic  7\ndid, people, moment, saint, friend, word, room, great, quite, ich, house, having, face, thought, went\n\nTopic  8\nwork, ayy, home, remember, gone, wake, stay, mmm, til, jump, beautiful, change, wait, doh, long\n\nTopic  9\nnigga, bitch, shit, fuck, bad, money, better, bout, hit, check, hoe, big, pop, lil, hey\n"
     ]
    }
   ],
   "source": [
    "display_topics(lda_tf, cv.get_feature_names(), 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nTopic  0\nrunaway, beggin, dreamed, wha, brokenhearted, dah, exception, downtown, husband, anniversary, barefoot, funkdafied, international, passing, jagger\n\nTopic  1\nich, shoo, doop, shoop, mmmmmm, vogue, clout, battlefield, nicht, woohoo, glamorous, monica, lounging, untouchable, nutty\n\nTopic  2\nbye, rockabye, dutty, diva, whoomp, kidd, mattered, ditty, soak, suicidal, reached, boi, tasted, delilah, eah\n\nTopic  3\naah, bent, woop, romantic, ron, coco, dura, confidence, traded, ella, dougie, eruption, location, motherfucking, skater\n\nTopic  4\nnigga, bitch, shit, money, fuck, gon, body, gotta, rock, ayy, dance, hit, party, bout, shake\n\nTopic  5\nhypnotized, excited, sha, heyy, desert, unwind, upper, romeo, fighter, aaa, peat, hopeless, quan, timber, natural\n\nTopic  6\ndiggin, kissin, coco, choo, circus, justify, ong, whistle, overwhelmed, daughter, insensitive, thurr, sneakin, titanium, draw\n\nTopic  7\nbreathin, dangerous, duh, halo, haunt, understanding, insist, swang, getta, ayer, damaged, zoom, speechless, rachel, lone\n\nTopic  8\ntonight, dream, believe, stay, really, forever, gone, better, try, long, inside, home, friend, true, wrong\n\nTopic  9\nque, begun, instrumental, quiero, para, como, sha, noche, psycho, una, woulda, drag, pero, wednesday, drift\n"
     ]
    }
   ],
   "source": [
    "display_topics(lda_tfidf, tfidf.get_feature_names(), 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}